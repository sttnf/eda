{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Comparison: Clean Data, EDA, Clustering, and Regression\n",
    "\n",
    "This notebook loads the dataset, cleans it (duplicates, missing values, outliers), performs EDA, segments models via clustering (with optimal k via silhouette), and optionally trains an optimized RandomForest to predict Price Rating. All artifacts (plots, CSVs) are saved in the `outputs/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = 'llm_comparison_dataset.csv'\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_CSV = 'llm_comparison_dataset_clean.csv'\n",
    "\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "print(f'Data path exists: {Path(DATA_PATH).exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "def report_duplicates(df: pd.DataFrame) -> int:\n",
    "    dupes = df.duplicated().sum()\n",
    "    print(f'Duplicate rows detected: {dupes}')\n",
    "    return dupes\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    before = len(df)\n",
    "    df2 = df.drop_duplicates().copy()\n",
    "    print(f'Removed {before - len(df2)} duplicate rows. New shape: {df2.shape}')\n",
    "    return df2\n",
    "\n",
    "def summarize_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    miss = df.isna().sum()\n",
    "    pct = (miss / len(df) * 100).round(2)\n",
    "    out = pd.DataFrame({'missing': miss, 'missing_%': pct})\n",
    "    return out[out['missing'] > 0].sort_values('missing', ascending=False)\n",
    "\n",
    "def convert_booleans_and_binary_cats(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Dict]]:\n",
    "    df2 = df.copy()\n",
    "    mappings: Dict[str, Dict] = {}\n",
    "    # Explicit mapping for common boolean/binary columns\n",
    "    explicit_map = {\n",
    "        True: 1, False: 0,\n",
    "        'True': 1, 'False': 0,\n",
    "        'Yes': 1, 'No': 0,\n",
    "        'Y': 1, 'N': 0,\n",
    "        'Open-Source': 1, 'Closed-Source': 0,\n",
    "        'Open Source': 1, 'Closed Source': 0,\n",
    "        'open': 1, 'closed': 0\n",
    "    }\n",
    "    for col in df2.columns:\n",
    "        ser = df2[col]\n",
    "        # direct bool\n",
    "        if ser.dtype == bool:\n",
    "            df2[col] = ser.astype(int)\n",
    "            mappings[col] = {True: 1, False: 0}\n",
    "            continue\n",
    "        if ser.dtype == 'object':\n",
    "            uniques = pd.Series(ser.dropna().unique())\n",
    "            if len(uniques) == 2:\n",
    "                # Try explicit map first\n",
    "                if all(u in explicit_map for u in uniques):\n",
    "                    m = {u: explicit_map[u] for u in uniques}\n",
    "                else:\n",
    "                    # Alphabetical mapping fallback\n",
    "                    sorted_vals = sorted(uniques.astype(str))\n",
    "                    m = {sorted_vals[0]: 0, sorted_vals[1]: 1}\n",
    "                df2[col] = ser.map(m).astype('Int64')\n",
    "                mappings[col] = m\n",
    "    return df2, mappings\n",
    "\n",
    "def impute_missing(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    df2 = df.copy()\n",
    "    imputations: Dict[str, float] = {}\n",
    "    for col in df2.columns:\n",
    "        if df2[col].isna().any():\n",
    "            if pd.api.types.is_numeric_dtype(df2[col]):\n",
    "                val = df2[col].median()\n",
    "                df2[col] = df2[col].fillna(val)\n",
    "                imputations[col] = float(val)\n",
    "            else:\n",
    "                # mode may be empty if all NaN; guard it\n",
    "                mode_vals = df2[col].mode(dropna=True)\n",
    "                if len(mode_vals) > 0:\n",
    "                    val = mode_vals.iloc[0]\n",
    "                else:\n",
    "                    val = 'Unknown'\n",
    "                df2[col] = df2[col].fillna(val)\n",
    "                imputations[col] = val if isinstance(val, (int, float)) else str(val)\n",
    "    return df2, imputations\n",
    "\n",
    "def iqr_cap_outliers(df: pd.DataFrame, factor: float = 1.5) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df2 = df.copy()\n",
    "    replaced_counts = {}\n",
    "    bounds = {}\n",
    "    for col in df2.select_dtypes(include=[np.number]).columns:\n",
    "        ser = df2[col]\n",
    "        q1 = ser.quantile(0.25)\n",
    "        q3 = ser.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        if pd.isna(iqr) or iqr == 0:\n",
    "            continue\n",
    "        lower = q1 - factor * iqr\n",
    "        upper = q3 + factor * iqr\n",
    "        capped = ser.clip(lower, upper)\n",
    "        replaced_counts[col] = int((ser != capped).sum())\n",
    "        bounds[col] = (float(lower), float(upper))\n",
    "        df2[col] = capped\n",
    "    counts_df = pd.DataFrame({'replaced': pd.Series(replaced_counts)}).sort_values('replaced', ascending=False)\n",
    "    return df2, counts_df\n",
    "\n",
    "def select_clustering_features(df: pd.DataFrame, preferred: Optional[List[str]] = None, min_features: int = 2) -> List[str]:\n",
    "    preferred = preferred or []\n",
    "    available = [c for c in preferred if c in df.columns]\n",
    "    if len(available) >= min_features:\n",
    "        return available\n",
    "    # Fallback: top-variance numeric features\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    if num.shape[1] >= min_features:\n",
    "        vars_ = num.var().sort_values(ascending=False)\n",
    "        return list(vars_.head(max(min_features, min(4, len(vars_)))).index)\n",
    "    return []\n",
    "\n",
    "def best_k_by_silhouette(X: np.ndarray, k_min: int = 2, k_max: int = 8, random_state: int = 42) -> Tuple[Optional[int], pd.DataFrame]:\n",
    "    n = X.shape[0]\n",
    "    k_max_eff = min(k_max, max(2, n - 1))\n",
    "    scores = []\n",
    "    for k in range(k_min, k_max_eff + 1):\n",
    "        try:\n",
    "            km = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
    "            labels = km.fit_predict(X)\n",
    "            # Silhouette needs at least 2 clusters and less than n unique labels\n",
    "            if len(set(labels)) < 2 or len(set(labels)) >= n:\n",
    "                continue\n",
    "            score = silhouette_score(X, labels)\n",
    "            scores.append({'k': k, 'silhouette': score})\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    if not df_scores.empty:\n",
    "        best_row = df_scores.loc[df_scores['silhouette'].idxmax()]\n",
    "        return int(best_row['k']), df_scores\n",
    "    return None, df_scores\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, title: str, save_path: Optional[Path] = None):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols) < 2:\n",
    "        print('Not enough numeric columns for correlation heatmap.')\n",
    "        return\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    corr = df[num_cols].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def pca_scatter(X: np.ndarray, labels: np.ndarray, title: str, save_path: Optional[Path] = None):\n",
    "    if X.shape[1] < 2:\n",
    "        print('Not enough dimensions for PCA scatter.')\n",
    "        return\n",
    "    pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "    X2 = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(x=X2[:,0], y=X2[:,1], hue=labels, palette='viridis', s=90, alpha=0.85)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def regression_features(df: pd.DataFrame, target: str, extra_exclude: Optional[List[str]] = None) -> List[str]:\n",
    "    exclude = set(extra_exclude or []) | {target}\n",
    "    # numeric only and non-constant\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    variances = num.var()\n",
    "    keep = [c for c in num.columns if c not in exclude and variances.get(c, 0) > 0]\n",
    "    return keep\n",
    "\n",
    "def evaluate_regression(y_true, y_pred) -> Dict[str, float]:\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MSE': float(mse), 'RMSE': rmse, 'R2': float(r2)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(DATA_PATH).exists(), f'Data file not found: {DATA_PATH}'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# Clean column names whitespace\n",
    "df.columns = df.columns.str.strip()\n",
    "print(df.shape)\n",
    "display(df.head(3))\n",
    "display(df.dtypes.to_frame('dtype').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = report_duplicates(df)\n",
    "if dupes > 0:\n",
    "    df = remove_duplicates(df)\n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Missing values and type fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values before:')\n",
    "display(summarize_missing(df))\n",
    "\n",
    "df, bin_maps = convert_booleans_and_binary_cats(df)\n",
    "if bin_maps:\n",
    "    print('Binary mappings applied:')\n",
    "    display(pd.DataFrame({k: pd.Series(v) for k, v in bin_maps.items()}))\n",
    "\n",
    "df, imputes = impute_missing(df)\n",
    "print('Imputations used:')\n",
    "display(pd.DataFrame.from_dict(imputes, orient='index', columns=['imputed_value']))\n",
    "\n",
    "print('Missing values after:')\n",
    "display(summarize_missing(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Outlier capping (IQR winsorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, outlier_counts = iqr_cap_outliers(df, factor=1.5)\n",
    "print('Outlier capping summary (number of values capped):')\n",
    "display(outlier_counts)\n",
    "outlier_counts.to_csv(OUTPUT_DIR / 'outlier_capping_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(CLEAN_CSV, index=False)\n",
    "print(f'Cleaned dataset saved to: {CLEAN_CSV}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) EDA: Descriptive statistics and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.describe(include='all')\n",
    "desc.to_csv(OUTPUT_DIR / 'descriptive_statistics.csv')\n",
    "display(desc.head())\n",
    "plot_correlation_heatmap(df, title='Correlation Matrix of LLM Metrics', save_path=OUTPUT_DIR / 'correlation_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Clustering: Optimal k via silhouette, PCA visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_features = ['Benchmark (MMLU)', 'Benchmark (Chatbot Arena)', 'Price / Million Tokens', 'Speed (tokens/sec)']\n",
    "cluster_feats = select_clustering_features(df, preferred_features, min_features=2)\n",
    "print('Selected clustering features:', cluster_feats)\n",
    "can_cluster = len(cluster_feats) >= 2 and len(df) >= 4\n",
    "if not can_cluster:\n",
    "    print('Not enough data/features to perform clustering. Skipping.')\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    Xc = scaler.fit_transform(df[cluster_feats])\n",
    "    k_best, sil_scores = best_k_by_silhouette(Xc, k_min=2, k_max=8, random_state=RANDOM_STATE)\n",
    "    display(sil_scores)\n",
    "    if not sil_scores.empty:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.lineplot(x='k', y='silhouette', data=sil_scores, marker='o')\n",
    "        plt.title('Silhouette scores by k')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'silhouette_scores.png', dpi=150)\n",
    "        plt.show()\n",
    "    if k_best is None:\n",
    "        print('Could not determine best k via silhouette; defaulting to k=3')\n",
    "        k_best = 3\n",
    "    print(f'Best k selected: {k_best}')\n",
    "    kmeans = KMeans(n_clusters=int(k_best), n_init=10, random_state=RANDOM_STATE)\n",
    "    df['Cluster'] = kmeans.fit_predict(Xc)\n",
    "    print('Cluster counts:')\n",
    "    display(df['Cluster'].value_counts())\n",
    "    # PCA scatter\n",
    "    pca_scatter(Xc, df['Cluster'].values, title='PCA Scatter of Clusters', save_path=OUTPUT_DIR / 'cluster_pca_scatter.png')\n",
    "    # Save clustered data preview\n",
    "    df.to_csv(OUTPUT_DIR / 'data_with_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Regression: Predict Price Rating with optimized RandomForest (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Price Rating'\n",
    "if target_col not in df.columns:\n",
    "    print(f'Target column \'{target_col}\' not found. Skipping regression.')\n",
    "else:\n",
    "    feature_cols = regression_features(df, target=target_col, extra_exclude=['Cluster'])\n",
    "    if len(feature_cols) < 2:\n",
    "        print('Not enough predictive features after filtering. Skipping regression.')\n",
    "    else:\n",
    "        X = df[feature_cols]\n",
    "        y = df[target_col]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "        rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        param_dist = {\n",
    "            'n_estimators': [100, 200, 300, 500],\n",
    "            'max_depth': [None, 5, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['auto', 'sqrt', 0.5, None]\n",
    "        }\n",
    "        search = RandomizedSearchCV(\n",
    "            rf, param_distributions=param_dist, n_iter=25, cv=3, scoring='r2',\n",
    "            random_state=RANDOM_STATE, n_jobs=-1, verbose=0\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_rf = search.best_estimator_\n",
    "        print('Best params:', search.best_params_)\n",
    "        y_pred = best_rf.predict(X_test)\n",
    "        metrics = evaluate_regression(y_test, y_pred)\n",
    "        print('Evaluation:', metrics)\n",
    "        pd.DataFrame([metrics]).to_csv(OUTPUT_DIR / 'regression_metrics.csv', index=False)\n",
    "        # Feature importances\n",
    "        fi = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': best_rf.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        display(fi)\n",
    "        fi.to_csv(OUTPUT_DIR / 'feature_importances.csv', index=False)\n",
    "        plt.figure(figsize=(9,6))\n",
    "        sns.barplot(data=fi, x='Importance', y='Feature', palette='magma')\n",
    "        plt.title('Feature Importances (RandomForest)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'feature_importances.png', dpi=150)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Save final analyzed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = OUTPUT_DIR / 'fully_analyzed_llm_data.csv'\n",
    "df.to_csv(final_path, index=False)\n",
    "print(f'Final dataset saved to: {final_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook complete. You can now explore the `outputs/` folder for generated figures and CSVs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

